{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "591197b0-4675-4d15-a0dd-6d77e22cc102",
   "metadata": {},
   "outputs": [],
   "source": [
    "## five data cleaning techniques\n",
    "# 1.Identify missing values, outliers, and inconsistencies.\n",
    "# 2.Clean missing data, remove duplicates, standardize text.\n",
    "# 3.Handle outliers and convert data types.\n",
    "# 4.Create new features, normalize data, and encode categorical variables.\n",
    "# 5.Save the cleaned and transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0df3a5ee-1889-4e7a-9cc4-86073a4b4dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Original Order</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15 Mar 2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>Bargara Beach</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Brooklyn Sauer</td>\n",
       "      <td>F</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>Tiger shark</td>\n",
       "      <td>Yahoo News, 3/15/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04 Mar 2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Old Man's, Waikiki</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Matthew White</td>\n",
       "      <td>M</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Tiger shark 8'</td>\n",
       "      <td>Surfer, 3/6/2024F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02 Mar-2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Rainbows, Oahu</td>\n",
       "      <td>Swimming</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>3' to 4' shark</td>\n",
       "      <td>Hawaii News Now, 3/4/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25 Feb-2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>Sandlnd Island, Jurian Bay</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>Tiger shark</td>\n",
       "      <td>WA Today, 2/26/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14 Feb-2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Vaitarna River, Palghar District</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Vicky Suresh Govari</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>Bull shark, 7'</td>\n",
       "      <td>Times of India, 2/14/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6964</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6965</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6966</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6967</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6968</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6969 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  Year        Type    Country              State  \\\n",
       "0     15 Mar 2024  2024  Unprovoked  AUSTRALIA         Queensland   \n",
       "1     04 Mar 2024  2024  Unprovoked        USA             Hawaii   \n",
       "2     02 Mar-2024  2024  Unprovoked        USA             Hawaii   \n",
       "3     25 Feb-2024  2024  Unprovoked  AUSTRALIA  Western Australia   \n",
       "4     14 Feb-2024  2024  Unprovoked      INDIA        Maharashtra   \n",
       "...           ...   ...         ...        ...                ...   \n",
       "6964          nan   nan         NaN        NaN                NaN   \n",
       "6965          nan   nan         NaN        NaN                NaN   \n",
       "6966          nan   nan         NaN        NaN                NaN   \n",
       "6967          nan   nan         NaN        NaN                NaN   \n",
       "6968          nan   nan         NaN        NaN                NaN   \n",
       "\n",
       "                              Location  Activity                 Name Sex Age  \\\n",
       "0                        Bargara Beach  Swimming       Brooklyn Sauer   F  13   \n",
       "1                   Old Man's, Waikiki   Surfing        Matthew White   M       \n",
       "2                       Rainbows, Oahu  Swimming                        F  11   \n",
       "3           Sandlnd Island, Jurian Bay                                  F  46   \n",
       "4     Vaitarna River, Palghar District   Fishing  Vicky Suresh Govari   M  32   \n",
       "...                                ...       ...                  ...  ..  ..   \n",
       "6964                               NaN                                          \n",
       "6965                               NaN                                          \n",
       "6966                               NaN                                          \n",
       "6967                               NaN                                          \n",
       "6968                               NaN                                          \n",
       "\n",
       "      ...         Species                     Source  pdf  \\\n",
       "0     ...     Tiger shark      Yahoo News, 3/15/2024  NaN   \n",
       "1     ...  Tiger shark 8'          Surfer, 3/6/2024F  NaN   \n",
       "2     ...  3' to 4' shark  Hawaii News Now, 3/4/2024  NaN   \n",
       "3     ...     Tiger shark        WA Today, 2/26/2024  NaN   \n",
       "4     ...  Bull shark, 7'  Times of India, 2/14/2024  NaN   \n",
       "...   ...             ...                        ...  ...   \n",
       "6964  ...             NaN                        NaN  NaN   \n",
       "6965  ...             NaN                        NaN  NaN   \n",
       "6966  ...             NaN                        NaN  NaN   \n",
       "6967  ...             NaN                        NaN  NaN   \n",
       "6968  ...             NaN                        NaN  NaN   \n",
       "\n",
       "                                           href formula href Case Number  \\\n",
       "0                                                   NaN  NaN         NaN   \n",
       "1                                                   NaN  NaN         NaN   \n",
       "2                                                   NaN  NaN         NaN   \n",
       "3                                                   NaN  NaN         NaN   \n",
       "4                                                   NaN  NaN         NaN   \n",
       "...                                                 ...  ...         ...   \n",
       "6964  http://sharkattackfile.net/spreadsheets/pdf_di...  NaN         NaN   \n",
       "6965  http://sharkattackfile.net/spreadsheets/pdf_di...  NaN         NaN   \n",
       "6966  http://sharkattackfile.net/spreadsheets/pdf_di...  NaN         NaN   \n",
       "6967  http://sharkattackfile.net/spreadsheets/pdf_di...  NaN         NaN   \n",
       "6968  http://sharkattackfile.net/spreadsheets/pdf_di...  NaN         NaN   \n",
       "\n",
       "     Case Number.1 Original Order Unnamed: 21 Unnamed: 22  \n",
       "0              NaN            NaN         NaN         NaN  \n",
       "1              NaN            NaN         NaN         NaN  \n",
       "2              NaN            NaN         NaN         NaN  \n",
       "3              NaN            NaN         NaN         NaN  \n",
       "4              NaN            NaN         NaN         NaN  \n",
       "...            ...            ...         ...         ...  \n",
       "6964           NaN            NaN         NaN         NaN  \n",
       "6965           NaN            NaN         NaN         NaN  \n",
       "6966           NaN            NaN         NaN         NaN  \n",
       "6967           NaN            NaN         NaN         NaN  \n",
       "6968           NaN            NaN         NaN         NaN  \n",
       "\n",
       "[6969 rows x 23 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Functions\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#remove the word \"Reported\" and keep the date\n",
    "def clean_dates(input_string):\n",
    "    input_data = str(input_string)\n",
    "    cleaned_string = re.sub(r'Reported\\s*', '', input_data)\n",
    "    return cleaned_string \n",
    "    \n",
    "#Replace two numbers which has & with average result of two numbers\n",
    "def clean_year_column(input_string):\n",
    "    input_data = str(input_string)\n",
    "    return re.sub(r'(\\d{4})\\.0', r'\\1', input_data)\n",
    "\n",
    "def replace_with_average(age_str):\n",
    "    # Regex to find \"x & y\"\n",
    "    age_str = str(age_str) \n",
    "    match = re.match(r'(\\d+)\\s*(?:and|or|to|&|/)\\s*(\\d+)', age_str)\n",
    "    \n",
    "    if match:\n",
    "        num1 = int(match.group(1))\n",
    "        num2 = int(match.group(2))\n",
    "        average = (num1 + num2) / 2\n",
    "        return average\n",
    "    return age_str\n",
    "\n",
    "#Find all numbers (including decimals) in the string\n",
    "def extract_numbers(value):\n",
    "    value = str(value).strip()\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', value)\n",
    "    \n",
    "    if numbers:\n",
    "        return int(round(float(numbers[0])))\n",
    "    return np.nan\n",
    "    \n",
    "#remove ½ and round up the numbers\n",
    "def clean_age(age_str):\n",
    "    age_str = str(age_str).strip()\n",
    "    match = re.findall(r'\\d+\\.?\\d*|\\d+½', age_str)\n",
    "    \n",
    "    if match:\n",
    "        processed_numbers = []\n",
    "        for num in match:\n",
    "            if '½' in num:\n",
    "                num = num.replace('½', '.5') #Eg. 6½ is round up as 6.5\n",
    "            processed_numbers.append(float(num))\n",
    "        processed_numbers = [num for num in processed_numbers]\n",
    "        \n",
    "        if processed_numbers:\n",
    "            return sum(processed_numbers) / len(processed_numbers)\n",
    "    \n",
    "    return None\n",
    "\n",
    "#Keep sex/gender as F and M and replace other with empty value\n",
    "def clean_gender_column(input_string):\n",
    "    input_str = str(input_string)\n",
    "    return re.sub(r'[^FM]', '', input_str)\n",
    "\n",
    "#Keep name replace female|nan|NaN with empty value\n",
    "def remove_female_and_nan(input_string):\n",
    "    input_string = str(input_string)\n",
    "    cleaned_str = re.sub(r'\\b(male|female|nan|NaN)\\b', '', input_string, flags=re.IGNORECASE)\n",
    "    cleaned_str = re.sub(r'\\s*,\\s*', ',', cleaned_str)  \n",
    "    cleaned_str = re.sub(r',+', ',', cleaned_str)       \n",
    "    cleaned_str = cleaned_str.strip(', ')\n",
    "    return cleaned_str\n",
    "    \n",
    "#########################################  Load and inspect raw data file ###########################################\n",
    "\n",
    "##Read excel file\n",
    "url = 'https://www.sharkattackfile.net/spreadsheets/GSAF5.xls'\n",
    "df = pd.read_excel(url)\n",
    "\n",
    "\n",
    "df['Date'] = df['Date'].apply(clean_dates)\n",
    "\n",
    "df['Year'] = df['Year'].apply(clean_year_column)\n",
    "\n",
    "############################################  Type Column  #########################################################\n",
    "df[\"Type\"] = df[\"Type\"].str.strip()\n",
    "df[\"Type\"] = df[\"Type\"].replace(['Unverified', 'Questionable', 'Under investigation'], 'Unconfirmed')\n",
    "df[\"Type\"] = df[\"Type\"].replace('?', np.nan)\n",
    "\n",
    "\n",
    "############################################  Activity Column  ####################################################\n",
    "\n",
    "df['Activity'] = df['Activity'].astype(str).replace('nan', '')\n",
    "df['Activity'] = df['Activity'].str.replace(r'^\\d+$', '', regex=True)\n",
    "\n",
    "\n",
    "############################################  Name Column  ####################################################\n",
    "df['Name'] = df['Name'].apply(remove_female_and_nan)\n",
    "\n",
    "############################################  Sex Column  #########################################################\n",
    "df['Sex'] = df['Sex'].apply(clean_gender_column)\n",
    "\n",
    "############################################  Age Column  #########################################################\n",
    "\n",
    "#Replace two numbers which has & with average result of two numbers\n",
    "df['Age'] = df['Age'].apply(replace_with_average)\n",
    "\n",
    "#Find all numbers (including decimals) in the string\n",
    "df['Age'] = df['Age'].apply(extract_numbers).astype(float).round().astype('Int64')\n",
    "\n",
    "#remove ½ and keep only numbers\n",
    "df['Age'] = df['Age'].apply(clean_age).astype(float).round().astype('Int64')\n",
    "\n",
    "#Replace <NA> with empty values\n",
    "df['Age'] = df['Age'].astype(str).replace('<NA>', '')\n",
    "\n",
    "############################################  Injury Column  #########################################################\n",
    "df[\"Injury\"] = df[\"Injury\"].str.strip()\n",
    "# rename to fatal if fatal is in string\n",
    "df[\"Injury\"] = df[\"Injury\"].apply(lambda x: \"Fatal\" if pd.notna(x) and \"FATAL\" in x.upper() and \"NOT FATAL\" not in x.upper() else x)\n",
    "\n",
    "# rename non Fatal if \"Fatal\" not in string\n",
    "df[\"Injury\"] = df[\"Injury\"].apply(lambda x: \"Non-Fatal\" if x != \"Fatal\" else x) \n",
    "\n",
    "############################################  Species Column  #########################################################\n",
    "df = df.rename(columns={'Species ': 'Species'})\n",
    "df = df.rename(columns={'original order': 'Original Order'})\n",
    "\n",
    "#Save cleaned data file\n",
    "df.to_csv('/Users/rishikeshdhokare/Documents/Ironhack/MiniProject/Quest2-Shark-Attacks/data-cleaning-pandas/sharkattacks_cleaned_data.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3f994-65f2-4919-be2c-fe3c36b497f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Unnamed: 11\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7621ed5-5b7d-49d0-9019-349dd01043fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Unnamed: 21\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9247f-1b99-4d1a-8715-4bd9dcf7f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Unnamed: 22\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5a97dd76-b10f-457e-ba95-e44e16107945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '  ',\n",
       " '   ',\n",
       " '\"After dark\"',\n",
       " '\"After lunch\"',\n",
       " '\"Early evening\"',\n",
       " '\"Evening\"',\n",
       " '\"Midday\"',\n",
       " '\"Night\"',\n",
       " '\"shortly before dusk\"',\n",
       " '--',\n",
       " '01:00',\n",
       " '0500',\n",
       " '06j00',\n",
       " '0830',\n",
       " '08:00',\n",
       " '09:15',\n",
       " '10j30',\n",
       " '10jh45',\n",
       " '11hoo',\n",
       " 1300,\n",
       " 1415,\n",
       " 1500,\n",
       " '15j45',\n",
       " '1600',\n",
       " '2 hours after Opperman',\n",
       " '2 hrs before sunset',\n",
       " '21:00',\n",
       " '22:00',\n",
       " '30 minutes after 1992.07.08.a',\n",
       " '8:04 pm',\n",
       " '9h00',\n",
       " 'A.M.',\n",
       " 'AM',\n",
       " 'After Dusk',\n",
       " 'After dusk',\n",
       " 'After midnight',\n",
       " 'After noon',\n",
       " 'Afternoon',\n",
       " 'Before daybreak',\n",
       " 'Dark',\n",
       " 'Dawn',\n",
       " 'Daybreak',\n",
       " 'Daytime',\n",
       " 'Dusk',\n",
       " 'Early  morning',\n",
       " 'Early Morning',\n",
       " 'Early afternoon',\n",
       " 'Early morning',\n",
       " 'Evening',\n",
       " 'FATAL  (Wire netting installed at local beaches after this incident.)',\n",
       " 'Just before dawn',\n",
       " 'Just before noon',\n",
       " 'Just before sundown',\n",
       " 'Late Afternoon',\n",
       " 'Late afternon',\n",
       " 'Late afternoon',\n",
       " 'Late morning',\n",
       " 'Late night',\n",
       " 'Lunchtime',\n",
       " 'Mid afternoon',\n",
       " 'Mid morning',\n",
       " 'Mid-morning',\n",
       " 'Midday',\n",
       " 'Midday.',\n",
       " 'Midnight',\n",
       " 'Morning',\n",
       " 'Morning ',\n",
       " 'N',\n",
       " 'Night',\n",
       " 'Nightfall',\n",
       " 'Noon',\n",
       " 'P.M.',\n",
       " 'Possibly same incident as 2000.08.21',\n",
       " 'Shortly after midnight',\n",
       " 'Sunset',\n",
       " 'X',\n",
       " 'dusk',\n",
       " 'night',\n",
       " '\\xa0 '}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "time_list = list(df[\"Time\"].unique())\n",
    "pattern = r'\\d{2}h\\d{2}'\n",
    "\n",
    "time_list = [item for item in time_list if isinstance(item, str) and re.search(pattern, item)]\n",
    "# time_list = [item.split() for item in time_list]\n",
    "\n",
    "# time_list = df['Time'].str.contains(r'\\d{2}h\\d{2}', regex=True)\n",
    "time_list\n",
    "\n",
    "\n",
    "replaced_list = []\n",
    "\n",
    "for time in time_list:\n",
    "    if \"-\" in time:\n",
    "        replaced_list.append(time)\n",
    "\n",
    "replaced_list\n",
    "\n",
    "df[\"Time\"] = df[\"Time\"].replace({'-16h30':'16h30',\n",
    " '09h00-10h00':'09h30',\n",
    " '14h00-15h00':'14h30',\n",
    " '14h00  -15h00':'14h30',\n",
    " '10h45-11h15':'11h00',\n",
    " '07h00 - 08h00':'07h30',\n",
    " '18h15-18h30':'18h30',\n",
    " '09h00 - 09h30':'09:15',\n",
    " '10h00 -- 11h00':'10h30',\n",
    " '09h00 -10h00':'09h30',\n",
    " '14h00 - 15h00':'14h40',\n",
    " '03h45 - 04h00':'04h00',\n",
    " '11h01 -time of ship sinking':'11h00',\n",
    " 'Ship aban-doned at 03h10':'03h00',\n",
    " '06h00 -- 07h00':'06h30',\n",
    " '17h00-18h00':'17h30',\n",
    " '19h00-20h00':'19h30'})\n",
    "\n",
    "df[\"Time\"].unique()\n",
    "\n",
    "replaced_list_1 = []\n",
    "\n",
    "\n",
    "for time in df[\"Time\"]:\n",
    "    # Check if the 'time' is not NaN and does not match the pattern using re.search()\n",
    "    if pd.notna(time) and not re.search(pattern, str(time)):  # Check for NaN and regex match\n",
    "        replaced_list_1.append(time)\n",
    "\n",
    "len(replaced_list_1)\n",
    "set(replaced_list_1)\n",
    "\n",
    "df[\"Time\"] = df[\"Time\"].replace({\n",
    " '\"After dark\"':'21h30',\n",
    " '\"After lunch\"':'13h00',\n",
    " '\"Early evening\"':'18h00',\n",
    " '\"Evening\"':'20h00',\n",
    " '\"Midday\"':'12h00',\n",
    " '\"Night\"':'22h00',\n",
    " '\"shortly before dusk\"':'06h00',\n",
    " 'After Dusk':'09h00',\n",
    " 'After dusk':'09h00',\n",
    " 'After midnight':'01h00',\n",
    " 'After noon':'13h00',\n",
    " 'Afternoon':'15h00',\n",
    " 'Before daybreak':'06h00',\n",
    " 'Dark':'22h00',\n",
    " 'Dawn':'20h00',\n",
    " 'Daybreak':'08h00',\n",
    " 'Daytime':'12h00',\n",
    " 'Dusk':'08:00',\n",
    " 'Early  morning':'07h00',\n",
    " 'Early Morning':'07h00',\n",
    " 'Early afternoon':'14h00',\n",
    " 'Early morning':'07h00',\n",
    " 'Evening':'20h00',\n",
    " 'Just before dawn':'20h00',\n",
    " 'Just before noon':'11h00',\n",
    " 'Just before sundown':'19h00',\n",
    " 'Late Afternoon':'17h00',\n",
    " 'Late afternon':'17h00',\n",
    " 'Late afternoon':'17h00',\n",
    " 'Late morning':'10h00',\n",
    " 'Late night':'23h00',\n",
    " 'Lunchtime':'12h00',\n",
    " 'Mid afternoon':'15h00',\n",
    " 'Mid morning':'15h00',\n",
    " 'Mid-morning':'15h00',\n",
    " 'Midday':'12h00',\n",
    " 'Midday.':'12h00',\n",
    " 'Midnight':'24h00',\n",
    " 'Morning':'08:00',\n",
    " 'Morning ':'08:00',\n",
    " 'Night':'22:00',\n",
    " 'Nightfall':'21:00',\n",
    " 'Noon':'12h00',\n",
    " 'Shortly after midnight':'01:00',\n",
    " 'Sunset':'20h00',\n",
    " 'dusk':'08h00',\n",
    " 'night':'22h00','01:00':'01h00',\n",
    " '0500':'05h00',\n",
    " '06j00':'22h00',\n",
    " '0830':'08h30',\n",
    " '08:00':'08h00',\n",
    " '09:15':'09h15',\n",
    " '10j30':'10h30',\n",
    " '10jh45':'10h45',\n",
    " '11hoo':'11h00',\n",
    " 1300:'13h00',\n",
    " 1415:'14h15',\n",
    " 1500:'15h00',\n",
    " '15j45':'15h45',\n",
    " '1600':'16h00',\n",
    " '20:00':'20h00',\n",
    " '21:00':'21h00',\n",
    " '22:00':'22h00',\n",
    " '8:04 pm':'20h00'})\n",
    "\n",
    "df[\"Time\"].unique()\n",
    "\n",
    "for time in df[\"Time\"]:\n",
    "    # Check if the 'time' is not NaN and does not match the pattern using re.search()\n",
    "    if pd.notna(time) and not re.search(pattern, str(time)):  # Check for NaN and regex match\n",
    "        replaced_list_1.append(time)\n",
    "\n",
    "set(replaced_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce29d7a-1076-4556-8a31-bca6dc5bf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\d{2}h\\d{2}'\n",
    "\n",
    "matched_items = [item for item in time_list if re.search(pattern, item)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f30ff2-c1f5-49f9-8c27-a8aea717a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findings\n",
    "\n",
    "1. Species column has most of the values as null\n",
    "2. Time stamp in Time coloumn is different\n",
    "3. Name column has values in of sex(male, female) other than the actual name\n",
    "4. Injury columns has many FATAL name injuries\n",
    "5. ALl rows in cloumns have some null values\n",
    "6. Age column should be numeric \n",
    "7. Age column has no numbers it is fllowed by string values \n",
    "8.  All the below columns are null/empty\n",
    "href               object\n",
    "Case Number        object\n",
    "Case Number.1      object\n",
    "original order    float64\n",
    "Unnamed: 21        object\n",
    "Unnamed: 22        object‚\n",
    "\n",
    "9. There are no lables for 3 columns-- Unnamed: 11 , Unnamed: 21, Unnamed: 22\n",
    "10. Many duplicate values in some of the columns\n",
    "\n",
    "---\n",
    "\"Dates\" \n",
    "Different formats, '15 Mar 2024' / '1883-1889' / nan / \"0\"\n",
    "---\n",
    "\"Year\"\n",
    "Just three weird values\n",
    "---\n",
    "\"Type\"\n",
    "Unprovoked', ' Provoked', 'Provoked', 'Questionable', 'Watercraft', 'Sea Disaster', nan, '?', 'Unconfirmed', 'Unverified', 'Invalid', 'Under investigation', 'Boat'\n",
    "---\n",
    "\"Country\"\n",
    "Country name USA not unified, some NaN values\n",
    "---\n",
    "\"Location\"\n",
    "Some numbers and NaN values instead of city/location names\n",
    "---\n",
    "\"Activity\"\n",
    "\n",
    "---\n",
    "\"Name\"\n",
    "A wild mix of different attributes\n",
    "---\n",
    "\"Sex\"\n",
    "Mostly M anf F, rest can probably be deleted\n",
    "---\n",
    "\"Age\"\n",
    "Some ranges and words, mostly numbers of ages though\n",
    "---\n",
    "\"Activity\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
